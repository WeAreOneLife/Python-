# 爬虫乱码处理

网页编码格式的信息通常放在响应对象的响应头中，使用requests模块的text属性从响应对象中提取网页源代码时，会从响应头中获取content-type字段的值，该值就包含编码格式的信息。例如，获取到content-type字段的值为“text/html; charset=utf-8”，其中的“charset=utf-8”表示此网页的编码格式为UTF-8，text属性就会对响应对象进行UTF-8格式的解码操作，我们看到的数据就不会出现乱码。

在谷歌浏览器中使用开发者工具查看某个网页的源代码，编码格式信息一般写在<meta>标签中，用get()函数获取到响应对象后，就可以通过对响应对象的encoding属性赋值，为响应对象指定这种编码格式。

还有一种方法是通过响应对象的apparent_encoding属性获取网页源代码中书写的编码格式信息，这样就不需要通过开发者工具查看<meta>标签中的编码格式信息。

此外，还有一种常见乱码是以“\u”开头的十六进制字符串，需要通过编码转换的方式来解码。
```python
str_16_1= "b'\\u4f60\\u597d'"
str_16_2 = str_16_1.encode('utf-8').decode('unicode_escape')  # 进行编码转换
print(str_16_2)  # 转换结果是“b'你好'”
```